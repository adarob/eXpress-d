#!/usr/bin/env python

import argparse
import itertools
from math import sqrt
import os.path
import re
from subprocess import Popen, PIPE
import sys
import threading
import time

# --------------------------------------------------------------------------------------------------
# NOTE: This script is a modified (and heavily simplified) version of the 'bin/run' script from
#       'https://github.com/amplab/spark-perf'.
# --------------------------------------------------------------------------------------------------

# The express-D project directory
proj_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
#sbt_cmd = "sbt/sbt"
sbt_cmd = "/usr/bin/sbt"

parser = parser = argparse.ArgumentParser(description='Run Express-D. Before running, '
    'edit the configuration file in express-D/config.')

parser.add_argument('--config-file', help='override default location of config file, must be a '
    'python file that ends in .py', default="%s/config/config.py" % proj_dir)

args = parser.parse_args()
assert args.config_file.endswith(".py"), "config filename must end with .py"

# Check if the config file exists.
assert os.path.isfile(args.config_file), ("Please create a config file called %s (you probably "
    "just want to copy and then modify %s/config/config.py.template)" % 
    (args.config_file, proj_dir))

print "Detected project directory: %s" % proj_dir
# Import the configuration settings from the config file.
print("Adding %s to sys.path" % os.path.dirname(args.config_file))
sys.path.append(os.path.dirname(args.config_file))
print("running 'import %s'" % os.path.basename(args.config_file).split(".")[0])
exec("import %s" % os.path.basename(args.config_file).split(".")[0])

# Setup environment variables.
os.environ["SPARK_HOME"] = config.SPARK_HOME
os.environ["EXPRESS_D_HOME"] = config.EXPRESS_D_HOME

# Determine what to build based on user-specified variables.
should_prep_spark = not config.SPARK_SKIP_PREP
should_prep_express_d = not config.EXPRESS_D_SKIP_PREP

# Run shell command and ignore output.
def run_cmd(cmd, exit_on_fail=True):
    if cmd.find(";") != -1:
        print("***************************")
        print("WARNING: the following command contains a semicolon which may cause non-zero return "
            "values to be ignored. This isn't necessarily a problem, but proceed with caution!")
    print(cmd)
    return_code = Popen(cmd, stdout=sys.stderr, shell=True).wait()
    if exit_on_fail:
        if return_code != 0:
            print "The following shell command finished with a non-zero returncode (%s): %s" % (
                return_code, cmd)
            sys.exit(-1)
    return return_code

# Run several commands in parallel, waiting for them all to finish.
# Expects an array of tuples, where each tuple consists of (command_name, exit_on_fail).
def run_cmds_parallel(commands):
    threads = []
    for (cmd_name, exit_on_fail) in commands:
        thread = threading.Thread(target=run_cmd, args=(cmd_name, exit_on_fail))
        thread.start()
        threads = threads + [thread]
    for thread in threads:
        thread.join()

# Return a command running cmd_name on host with proper SSH configs.
def make_ssh_cmd(cmd_name, host):
    return "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 %s '%s'" % (host, cmd_name)

# Return a command which copies the supplied directory to the given host.
def make_rsync_cmd(dir_name, host):
    return ('rsync --delete -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5" -az "%s/" '
        '"%s:%s"') % (dir_name, host, os.path.abspath(dir_name))

# Delete all files in the given directory on the specified hosts.
def clear_dir(dir_name, hosts):
    assert dir_name != "" and dir_name != "/", ("Attempted to delete directory '%s/*', halting "
        "rather than deleting entire file system.") % dir_name
    if config.PROMPT_FOR_DELETES:
        response = raw_input("\nAbout to remove all files and directories under %s on %s, is "
            "this ok? [y, n] " % (dir_name, hosts))
        if response != "y":
            return
    run_cmds_parallel([(make_ssh_cmd("rm -r %s/*" % dir_name, host), False) for host in hosts])

# Ensures that no executors are running on Spark slaves. Executors can continue to run for some
# time after a shutdown signal is given due to cleaning up temporary files.
def ensure_spark_stopped_on_slaves(slaves):
    stop = False
    while not stop:
        cmd = "ps -ef |grep -v grep |grep ExecutorBackend"
        ret_vals = map(lambda s: run_cmd(make_ssh_cmd(cmd, s), False), slaves)
        if 0 in ret_vals:
            print "Spark is still running on some slaves ... sleeping for 10 seconds"
            time.sleep(10)
        else:
            stop = True

# Get a list of slaves by parsing the slaves file in SPARK_CONF_DIR.
slaves_file_raw = open("%s/slaves" % config.SPARK_CONF_DIR, 'r').read().split("\n")
slaves_list = filter(lambda x: not x.startswith("#") and not x is "", slaves_file_raw)

# Prepare Spark.
if should_prep_spark:
    # Assumes that the preexisting 'spark' directory is valid.
    if not os.path.isdir("spark"):
        # Clone Spark.
        print("Git cloning Spark...")
        run_cmd("git clone %s spark %s" % config.SPARK_GIT_REPO, config.SPARK_HOME)

    # Package the fetched Spark source.
    run_cmd("%s clean package" % sbt_cmd)

    # Copy Spark configuration files to new directory.
    print("Copying all files from %s to %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))
    assert os.path.exists("%s/spark-env.sh" % config.SPARK_CONF_DIR), \
        "Could not find required file %s/spark-env.sh" % config.SPARK_CONF_DIR
    assert os.path.exists("%s/slaves" % config.SPARK_CONF_DIR), \
        "Could not find required file %s/slaves" % config.SPARK_CONF_DIR
    run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

    # Change back to 'proj_dir' directory.
    os.chdir("..")
    print("Done preparing Spark")
else:
    # TODO(andy): Make this check for the jar we will be including on the
    #             classpath (as part of work Patrick is doing), instead of
    #             just looking for spark/target.

    print()
    #assert os.path.exists(
    #    "%s/target" % config.SPARK_HOME), ("You chose to skip Spark prep, but we can't since " +
    #    "%s/target does not exist, so Spark needs to be rebuilt/packaged." % config.SPARK_HOME)
    #print("Skipping preparation tasks for Spark (i.e, git clone or update, checkout, build, " +
    #    "copy conf files).")

# There is a Spark jar at this point. Build Express-D, if needed.
spark_work_dir = "%s/spark/work" % proj_dir
if os.path.exists(spark_work_dir):
    # Clear the 'perf-tests/spark/work' directory beforehand, since that may contain scheduler logs
    # from previous jobs. The directory could also contain a spark-perf-tests-assembly.jar that may
    # interfere with subsequent 'sbt assembly' for Spark perf.
    clear_dir(spark_work_dir, ["localhost"])

if should_prep_express_d:
    # Make sure we're in the right directory.
    run_cmd("cd %s" % proj_dir)
    run_cmd("%s clean assembly" % sbt_cmd)
    print("Done assembling Express-D jar.")
else:
    express_d_jar_path = "%s/target/express-d-assembly.jar" % proj_dir
    assert os.path.exists(express_d_jar_path), ("You tried to skip packaging the Express-D " +
        "source, but %s cannot be found") % express_d_jar_path

assert 0 == 1

# Sync the whole directory to slaves.
print("Syncing the test directory to the slaves.")
run_cmds_parallel([(make_rsync_cmd(proj_dir, slave), True) for slave in slaves_list])

# Set Spark Java Options (from config.py)
new_env = os.environ.copy()

new_env["SPARK_HOME"] = config.SPARK_HOME
new_env["EXPRESS_D_HOME"] = config.EXPRESS_D_HOME

scala_cmd_classpath = "%s/target/express-d-assembly.jar" % proj_dir

# Run all tests specified in 'tests_to_run', a list of 5-element tuples. See the 'Test Setup'
# section in config.py.template for more info.
# Results are written as CSVs to 'output_filename'.
def run_express(scala_cmd_classpath, tests_to_run, test_group_name, output_filename):
    out_file = open(output_filename, 'w')
    num_tests_to_run = len(tests_to_run)

    output_divider_string = "\n--------------------------------------------------------------------"
    print(output_divider_string)
    print("Running %d tests in %s.\n" % (num_tests_to_run, test_group_name))

    for short_name, test_cmd, scale_factor, java_opt_sets, opt_sets in tests_to_run:
        print(output_divider_string)
        print("Running test command: '%s' ..." % test_cmd)
        # Run a test for all combinations of the OptionSets given, then capture
        # and print the output.
        java_opt_set_arrays = [i.to_array(scale_factor) for i in java_opt_sets]
        opt_set_arrays = [i.to_array(scale_factor) for i in opt_sets]
        for java_opt_list in itertools.product(*java_opt_set_arrays):
            for opt_list in itertools.product(*opt_set_arrays):
                ensure_spark_stopped_on_slaves(slaves_list)
                results_token = "results: "
                # TODO(andy): Add a timout on the subprocess.
                cmd = "%s %s -cp %s %s %s %s" % (config.SCALA_CMD, " ".join(java_opt_list),
                    scala_cmd_classpath, test_cmd, config.SPARK_CLUSTER_URL, " ".join(opt_list))
                print("\nrunning command: %s\n" % cmd)
                output = Popen(cmd, stdout=PIPE, shell=True, env=new_env).stdout.read()
                if results_token not in output:
                    print("Test did not produce expected results. Output was:")
                    print(output)
                    sys.exit(1)
                result_line = filter(lambda x: results_token in x, output.split("\n"))[0]

                result_string = result_line.replace(results_token, "").split(",")
                print(result_string)
                out_file.write(result_string + "\n")
                sys.stdout.flush()
                out_file.flush()

    print("\nFinished running %d tests in %s.\nSee CSV output in %s" %
        (num_tests_to_run, test_group_name, output_filename))
    print(output_divider_string)

# Run Express-D
run_express(scala_cmd_classpath, config.EXPRESS_RUN, "Express-D", config.TIMINGS_OUTPUT_FILE)

print("Finished running Express-D")
